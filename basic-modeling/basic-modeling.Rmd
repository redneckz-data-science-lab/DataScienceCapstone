---
title: "Smart Keyboard Basic Modeling"
author: "Alexander Alexandrov"
date: "Friday, June 03, 2016"
output:
    html_document:
        code_folding: hide
---

## Introduction

The goal here is to build first simple model for the relationship between words. This is the first step in building a predictive text mining application.

Tasks to accomplish:

* Build basic n-gram model - using the exploratory analysis, build a basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words.
* Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

Questions to consider:

1. How to efficiently store an n-gram model (may be *Markov Chains*)?
2. How to use the knowledge about word frequencies to make model smaller and more efficient?
3. How many parameters is really needed (i.e. how big is n in n-gram model)?
4. Simple ways to "smooth" the probabilities (for example, giving all n-grams a non-zero probability even if they aren't observed in the data) ?
5. How to evaluate whether model is any good?
6. How to use *backoff* models to estimate the probability of unobserved n-grams?

## Exploratory Analysis Results Overview

```{r, echo=F, warning=F, message=F}
source("../common/partitioned-corpus.R", chdir=T)
source("../common/ngram-freq-table.R", chdir=T)
```

```{r}
(function() {
    ngram.freq.files <- list.files("../exploratory-analysis/output/freq-table",
                                   pattern=".rds$", full.names=T)
    file.count <- length(ngram.freq.files)
    group.count <- 4L # Memory/performance trade-off
    group.size <- ceiling(file.count / group.count)
    ngram.freq.files.groups <<- split(ngram.freq.files,
                                      f=rep(1:group.count,
                                            each=group.size)[1:file.count])
})()
```

Training data contains a lot of very rare n-grams, so can be cleaned after loading. For example, rare (frequency equals to one) unigrams and bigrams can be discarded. Because 3-grams and 4-grams are outnumbered.

```{r}
    CombineNgramFreqTablesGroups <- function(files.groups) {
        ngram.freq.table <- ComputeNgramFreqTablesGroup(files.groups[[1L]])
        for (files.group in files.groups[-1L]) {
            ngram.freq.table <- MergeNGramFreqTables(
                    ngram.freq.table,
                    ComputeNgramFreqTablesGroup(files.group))
        }
        return(ngram.freq.table)
    }
    ComputeNgramFreqTablesGroup <- function(files.group) {
        ngram.freq.table <- CombinePartitions(files.group,
                                              reduce.f=MergeNGramFreqTables)
        return(ngram.freq.table[(n < 3L) & (freq > 1L)])
    }
```

```{r}
if (!file.exists("./output/train.freq.table.rds")) {
    train.freq.table <- CombineNgramFreqTablesGroups(ngram.freq.files.groups)
    saveRDS(train.freq.table, "./output/train.freq.table.rds")
} else {
    train.freq.table <- readRDS("./output/train.freq.table.rds")
}
train.freq.table[, freq := as.integer(freq)] # Adjust type
setkey(train.freq.table, first.words, last.word, n)
summary(train.freq.table)
```

Test data should contain only the most frequent n-grams.

```{r}
if (!file.exists("./output/test.freq.table.rds")) {
    set.seed(123456L)
    test.files.group <- sapply(ngram.freq.files.groups,
                               function(files.group) sample(files.group, 1L))
    test.freq.table <- ComputeNgramFreqTablesGroup(test.files.group)
    test.freq.table <- test.freq.table[n > 1L, .SD[which.max(freq)],
                                       by=first.words]
    saveRDS(test.freq.table, "./output/test.freq.table.rds")
} else {
    test.freq.table <- readRDS("./output/test.freq.table.rds")
}
test.freq.table[, freq := as.integer(freq)] # Adjust type
setkey(test.freq.table, first.words, last.word, n)
summary(test.freq.table)
```

0. Significant part of dictionary consists of very rare words. So dictionary can be reduced.
1. For about **6 thousand words** is enough to cover **80%** of corpora.
2. Most efficient model could cover only **71%** of the language.
3. Stemming can help to increase coverage. But this is complicated technique.

Exploratory analysis report is located [here](http://rpubs.com/redneckz/smart-keyboard-exploratory-data-analysis).

## Model Effectiveness Criteria

Model is expected to work in mobile environment. That's why memory usage and latency are the most important criteria. Let's fix the following constraints:

0. **Accuracy** > 50%.
1. **Memory usage** < 1 Gb.
2. **Latency** < 3 sec.

**Accuracy** can be estimated by means of splitting into training and testing sets. Or by means of more complicated techniques. For example, *Cross Validation*.

## Model Common Parameters

> With n-gram models it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.

According to this approach, **5-grams** dictionary can be some kind of **overkill** relative to the **mobile domain**.

## Handling Unseen N-grams

If something entered by user is out of dictionary scope. Than it should be cleaned up and registered in dictionary. So probability/frequency should be choosed for new items.

> There are problems of balance weight between infrequent grams (for example, if a proper name appeared in the training data) and frequent grams. Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts. Pseudocounts are generally motivated on Bayesian grounds.

> In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before -- the zero-frequency problem.

Different *smoothing* approaches exist:

* Linear interpolation.
* Good–Turing discounting.
* Witten–Bell discounting.
* Lidstone's smoothing.
* Katz's back-off model (trigram).
* Kneser–Ney smoothing.

## Simple Prediction Model

Let's start with some simple model. Without counts smoothing.

### Model Overview

Algorithm steps:

0. Order n-gram dictionary by frequency.
1. Clean up provided query (the same way corpora has been cleaned up).
2. Tokenize and compute number of words.
3. Choose *n* (for *n-gram*) equals to number of words plus one word.
4. Search for appropriate *n-grams*.
5. If nothing found (or less than some coefficient), remove first word from query, and go to the step 3.
6. Extract last words from found *n-grams* and order them by frequency.

```{r, echo=F, message=F, warning=F}
source("../common/predict-next-words.R", chdir=T)
```

### Simple Model Smoke Testing

Testing phrase: *I cant wait*

Predicted words:

```{r}
PredictNextWords(train.freq.table, "I cant wait",
                 result.prepare.strategy=function(result) result$last.word)
```

### Simple Model Profiling

```{r}
ProfilePredictor <- function(predictor, ngram.freq, queries, max.ngram.count=10L) {
    tmp.file <- tempfile()
    sampling.time <- sapply(queries, FUN=function(query) {
        Rprof(tmp.file)
        do.call(predictor, list(ngram.freq=ngram.freq, query.text=query,
                                max.ngram.count=max.ngram.count))
        Rprof(NULL)
        return(summaryRprof(tmp.file)$sampling.time)
    })
    unlink(tmp.file)
    return(sampling.time)
}
```

#### Memory Usage

```{r}
format(object.size(train.freq.table), units="MB")
```

Memory usage is small enough.

#### Performance

```{r}
ngram.testing.queries <- c("first", "just", "cant", "will", "love",
                           "good luck", "one day", "look like", "good morning", 
                           "can get",
                           "cant wait see", "dont even know",
                           "new york city", "happy new year",
                           "dont feel like")
ngram.sampling.time <- ProfilePredictor(PredictNextWords,
                                        train.freq.table,
                                        queries=ngram.testing.queries)
print(paste(round(mean(ngram.sampling.time), 3), "seconds"))
```

So average latency is acceptable in most cases.

### Simple Model Accuracy

N-grams approach has fundamental disadvantage in context of unseen terms. So accuracy is estimated by weak rule: if expected word is contained in top ten predictions, such predictin is considered valid.

```{r}
EstimatePredictorAccuracy <- function(predictor,
                                      train.freq.table,
                                      test.freq.table,
                                      max.ngram.count=10L) {
    assert <- function(first.words, expected.word) {
        next.words.freq.table <- predictor(ngram.freq=train.freq.table,
                                           query.text=first.words,
                                           max.ngram.count=max.ngram.count)
        return(any(expected.word == next.words.freq.table$last.word))
    }
    assertion.result <- test.freq.table[, assert(first.words, last.word),
                                        by=1:nrow(test.freq.table)]$V1
    return(sum(assertion.result) / length(assertion.result))
}
```

Accuracy (percent):
```{r}
format(EstimatePredictorAccuracy(PredictNextWords,
                                 train.freq.table,
                                 test.freq.table) * 100,
       digits=3L)
```

### Simple Model Quiz

```{r}
test.quiz.1 <- list("The guy in front of me just bought a pound of bacon, a bouquet, and a case of" = c("pretzels", "soda", "beer", "cheese"),
                    "You're the reason why I smile everyday. Can you follow me please? It would mean the" = c("most", "world", "universe", "best"),
                    "Hey sunshine, can you follow me and make me the" = c("bluest", "saddest", "happiest", "smelliest"),
                    "Very early observations on the Bills game: Offense still struggling but the" = c("referees", "defense", "players", "crowd"),
                    "Go on a romantic date at the" = c("movies", "grocery", "beach", "mall"),
                    "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my" = c("phone", "horse", "way", "motorcycle"),
                    "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some" = c("years", "weeks", "time", "thing"),
                    "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little" = c("ears", "fingers", "toes", "eyes"),
                    "Be grateful for the good times and keep the faith during the" = c("hard", "worse", "sad", "bad"),
                    "If this isn't the cutest thing you've ever seen, then you must be" = c("insensitive", "asleep", "callous", "insane"))
```

```{r}
TestPredictorAgainstQuiz <- function(predictor, train.freq.table,
         test.quiz,
         max.ngram.count=500L) {
    res <- lapply(names(test.quiz), FUN=function(query) {
        expected.next.words <- test.quiz[[query]]
        next.words.freq.table <- predictor(ngram.freq=train.freq.table,
                                           query.text=query,
                                           max.ngram.count=max.ngram.count)
        return(next.words.freq.table[, .(next.words=intersect(last.word,
                                                              expected.next.words),
                                         rating.position=which(last.word %in%
                                                                   expected.next.words))])
    })
    names(res) <- names(test.quiz)
    return(res)
}
```

```{r}
simple.predict.quiz.result <- TestPredictorAgainstQuiz(PredictNextWords,
                                                       train.freq.table,
                                                       test.quiz.1)
print(simple.predict.quiz.result)
```

## Kneser-Ney Based Model

> Kneser-Ney evolved from absolute-discounting interpolation, which makes use of both higher-order (i.e., higher-n) and lower-order language models, reallocating some probability mass from 4-grams or 3-grams to simpler unigram models.

```{r}
source("../common/kneser-ney.R", chdir=T)
```

```{r}
if (!file.exists("./output/kn.train.freq.table.rds")) {
    kn.train.freq.table <- SmoothNgramFreqTableByKneserNey(train.freq.table)
    saveRDS(kn.train.freq.table, "./output/kn.train.freq.table.rds")
} else {
    kn.train.freq.table <- readRDS("./output/kn.train.freq.table.rds")
}
```

### Kneser-Ney Model Smoke Testing

Testing phrase: *I cant wait*

Predicted words:

```{r}
PredictNextWordsByKneserNey(kn.train.freq.table, "I cant wait",
                            result.prepare.strategy=function(result)
                                result$last.word)
```

### Kneser-Ney Model Profiling

#### Memory Usage

```{r}
format(object.size(kn.train.freq.table), units="MB")
```

Memory usage is small enough.

#### Performance

```{r}
ngram.sampling.time <- ProfilePredictor(PredictNextWordsByKneserNey,
                                        kn.train.freq.table,
                                        queries=ngram.testing.queries)
print(paste(round(mean(ngram.sampling.time), 3), "seconds"))
```

So average latency is acceptable in most cases.

### Kneser-Ney Model Quiz

```{r}
kn.predict.quiz.result <- TestPredictorAgainstQuiz(PredictNextWordsByKneserNey,
                                                   kn.train.freq.table,
                                                   test.quiz.1)
print(kn.predict.quiz.result)
```

### Compare Kneser-Ney to Simple Model by Quiz

```{r}
ComparePredictorsByQuiz <- function(quiz.result.a, quiz.result.b) {
    return(lapply(names(quiz.result.a), function(query) {
        a <- quiz.result.a[[query]][1]
        b <- quiz.result.b[[query]][1]
        return(data.frame(a=a$next.words, b=b$next.words,
                          rating.delta=a$rating.position - b$rating.position))
    }))
}
```

```{r}
ComparePredictorsByQuiz(simple.predict.quiz.result, kn.predict.quiz.result)
```

Delta rating field shows difference in prediction result position between simple predictor and Kneser-Ney based predictor. The closer position to the begining of the list, the better algorithm.

According to comparison results Kneser-Ney smoothing has no effect in most cases.
