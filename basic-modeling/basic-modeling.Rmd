---
title: "Smart Keyboard Basic Modeling"
author: "Alexander Alexandrov"
date: "Friday, June 03, 2016"
output:
    html_document:
        code_folding: hide
---

## Introduction

The goal here is to build first simple model for the relationship between words. This is the first step in building a predictive text mining application.

Tasks to accomplish:

* Build basic n-gram model - using the exploratory analysis, build a basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words.
* Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

Questions to consider:

1. How to efficiently store an n-gram model (may be *Markov Chains*)?
2. How to use the knowledge about word frequencies to make model smaller and more efficient?
3. How many parameters is really needed (i.e. how big is n in n-gram model)?
4. Simple ways to "smooth" the probabilities (for example, giving all n-grams a non-zero probability even if they aren't observed in the data) ?
5. How to evaluate whether model is any good?
6. How to use *backoff* models to estimate the probability of unobserved n-grams?

## Exploratory Analysis Results Overview

```{r, echo=F, warning=F, message=F}
source("../common/partitioned-corpus.R", chdir=T)
source("../common/ngram-freq-table.R", chdir=T)
```

```{r}
ngram.freq.files <- list.files("../exploratory-analysis/output/freq-table", pattern=".rds$", full.names=T)
file.count <- length(ngram.freq.files)
group.count <- 20
group.size <- ceiling(file.count / group.count)
ngram.freq.files.groups <- split(ngram.freq.files,
                                 f=rep(1:group.count,
                                       each=group.size)[1:file.count])
```

Training data contains a lot of very rare n-grams, so can be cleaned after loading.

```{r}
    CombineNgramFreqTablesGroups <- function(files.groups) {
        ngram.freq.table <- ComputeNgramFreqTablesGroup(files.groups[1][[1]])
        for (files.group in files.groups[-1]) {
            ngram.freq.table <- MergeNGramFreqTables(
                    ngram.freq.table,
                    ComputeNgramFreqTablesGroup(files.group))
        }
        return(ngram.freq.table[freq > 1])
    }
    ComputeNgramFreqTablesGroup <- function(files.group) {
        print(files.group)
        ngram.freq.table <- CombinePartitions(files.group,
                                              reduce.f=MergeNGramFreqTables)
        return(ngram.freq.table[freq > 1])
    }
```

```{r}
if (!file.exists("./output/train.freq.table.rds")) {
    train.freq.table <- CombineNgramFreqTablesGroups(ngram.freq.files.groups)
    saveRDS(train.freq.table, "./output/train.freq.table.rds")
} else {
    train.freq.table <- readRDS("./output/train.freq.table.rds")
}
summary(train.freq.table)
```

Test data should contain only the most frequent n-grams.

```{r}
if (!file.exists("./output/test.freq.table.rds")) {
    test.freq.table <- CombinePartitions(cv.ngram.freq.files[[2]],
                                         reduce.f=MergeNGramFreqTables)
    test.freq.table <- test.freq.table[n > 1 & freq > 1][, .SD[which.max(freq)],
                                                         by=first.words]
    saveRDS(test.freq.table, "./output/test.freq.table.rds")
} else {
    test.freq.table <- readRDS("./output/test.freq.table.rds")
}
summary(test.freq.table)
```

0. Significant part of dictionary consists of very rare words. So dictionary can be reduced.
1. For about **6 thousand words** is enough to cover **80%** of corpora.
2. Most efficient model could cover only **71%** of the language.
3. Stemming can help to increase coverage. But this is complicated technique.

Exploratory analysis report is located [here](http://rpubs.com/redneckz/smart-keyboard-exploratory-data-analysis).

## Model Effectiveness Criteria

Model is expected to work in mobile environment. That's why memory usage and latency are the most important criteria. Let's fix the following constraints:

0. **Accuracy** > 50%.
1. **Memory usage** < 1 Gb.
2. **Latency** < 3 sec.

**Accuracy** can be estimated by means of splitting into training and testing sets. Or by means of more complicated techniques. For example, *Cross Validation*.

## Model Common Parameters

> With n-gram models it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.

According to this approach, **5-grams** dictionary can be some kind of **overkill** relative to the **mobile domain**.

## Handling Unseen N-grams

If something entered by user is out of dictionary scope. Than it should be cleaned up and registered in dictionary. So probability/frequency should be choosed for new items.

> There are problems of balance weight between infrequent grams (for example, if a proper name appeared in the training data) and frequent grams. Also, items not seen in the training data will be given a probability of 0.0 without smoothing. For unseen but plausible data from a sample, one can introduce pseudocounts. Pseudocounts are generally motivated on Bayesian grounds.

> In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before -- the zero-frequency problem.

Different *smoothing* approaches exist:

* Linear interpolation.
* Good–Turing discounting.
* Witten–Bell discounting.
* Lidstone's smoothing.
* Katz's back-off model (trigram).
* Kneser–Ney smoothing.

## Simple Prediction Model

Let's start with some simple model. Without counts smoothing.

### Model Overview

Algorithm steps:

0. Order n-gram dictionary by frequency.
1. Clean up provided query (the same way corpora has been cleaned up).
2. Tokenize and compute number of words.
3. Choose *n* (for *n-gram*) equals to number of words plus one word.
4. Search for appropriate *n-grams*.
5. If nothing found (or less than some coefficient), remove first word from query, and go to the step 3.
6. Extract last words from found *n-grams* and order them by frequency.

```{r, echo=F, message=F, warning=F}
library("stringi")
source("../common/clean-corpus.R", chdir=T)
```

```{r}
PredictNextWords <- function(ngram.freq, query, max.ngram.count=5) {
    query.corpus <- CleanCorpus(VCorpus(VectorSource(query)))
    query.corpus.content <- query.corpus[[1]]$content
    query.words <- stri_extract_all_words(query.corpus.content)[[1]]
    resulting.ngram.freq <- FindNGrams(ngram.freq, query.words, max.ngram.count)
    return(resulting.ngram.freq$last.word)
}
FindNGrams <- function(ngram.freq, query.words, max.ngram.count,
                       result=data.table()) {
    if ((length(query.words) == 0) || (nrow(result) >= max.ngram.count)) {
        return(head(result, n=max.ngram.count))
    }
    target.n <- length(query.words) + 1
    target.query <- paste(query.words, collapse=" ")
    target.ngram.freq <- ngram.freq[(target.n == n) & (target.query == first.words)]
    return(FindNGrams(ngram.freq, tail(query.words, n=-1), max.ngram.count,
                      rbind(result, target.ngram.freq)))
}
```

### Simple Model Smoke Testing

Testing phrase: *I cant wait*

Predicted words:

```{r}
PredictNextWords(train.freq.table, "I cant wait", max.ngram.count=10)
```

### Simple Model Profiling

```{r}
ProfilePredictor <- function(predictor, ngram.freq, queries, max.ngram.count=10) {
    tmp.file <- tempfile()
    sampling.time <- sapply(queries, FUN=function(query) {
        Rprof(tmp.file)
        do.call(predictor, list(ngram.freq=ngram.freq, query=query,
                max.ngram.count=max.ngram.count))
        Rprof(NULL)
        return(summaryRprof(tmp.file)$sampling.time)
    })
    unlink(tmp.file)
    return(sampling.time)
}
```

#### Memory Usage

```{r}
format(object.size(train.freq.table), units="MB")
```

Memory usage is small enough.

#### 2-Gram Performance

```{r}
gram2.testing.queries <- c("first", "just", "cant", "will", "love")
gram2.sampling.time <- ProfilePredictor(PredictNextWords,
                                        train.freq.table,
                                        queries=gram2.testing.queries)
print(paste(mean(gram2.sampling.time), "seconds"))
```

#### 3-Gram Performance

```{r}
gram3.testing.queries <- c("good luck", "one day", "look like", "good morning", 
                           "can get")
gram3.sampling.time <- ProfilePredictor(PredictNextWords,
                                        train.freq.table,
                                        queries=gram3.testing.queries)
print(paste(mean(gram3.sampling.time), "seconds"))
```

#### 4-Gram Performance

```{r}
gram4.testing.queries <- c("cant wait see", "dont even know",
                           "new york city", "happy new year",
                           "dont feel like")
gram4.sampling.time <- ProfilePredictor(PredictNextWords, 
                                        train.freq.table,
                                        queries=gram4.testing.queries)
print(paste(mean(gram4.sampling.time), "seconds"))
```

So average latency is acceptable in major cases.

### Simple Model Accuracy

N-grams approach has fundamental disadvantage in context of unseen terms. So accuracy is estimated by weak rule: if expected word is contained in top ten predictions, such predictin is considered valid.

```{r}
EstimatePredictorAccuracy <- function(predictor,
                                      train.freq.table,
                                      test.freq.table,
                                      max.ngram.count=10) {
    assert <- function(first.words, expected.word) {
        next.words <- predictor(ngram.freq=train.freq.table,
                                query=first.words,
                                max.ngram.count=max.ngram.count)
        return(any(expected.word == next.words))
    }
    assertion.result <- test.freq.table[, assert(first.words, last.word),
                                        by=1:nrow(test.freq.table)]$V1
    return(sum(assertion.result) / length(assertion.result))
}
```

Accuracy (percent):
```{r}
format(EstimatePredictorAccuracy(PredictNextWords,
                                 train.freq.table,
                                 test.freq.table) * 100,
       digits=3)
```

## Kneser-Ney

> Kneser-Ney evolved from absolute-discounting interpolation, which makes use of both higher-order (i.e., higher-n) and lower-order language models, reallocating some probability mass from 4-grams or 3-grams to simpler unigram models.

```{r}

```
